---
title: HTTP/2.0
date: 2020-08-09
categories: HTTP
tags: HTTP
toc: true
thumbnail: https://gitee.com/heptaluan/backups/raw/master/cdn/cover/11.jpg
---

最近在梳理 `HTTP` 协议相关内容，当梳理到追加协议相关内容的时候发现 `HTTP/2.0` 所涉及到的内容篇幅还是有些多的，所以就单独提取出来，更多相关内容可以参考 [HTTP 协议梳理](http://localhost:4000/2020/09/01/HTTP/10/) 和 [前端知识体系整理](https://heptaluan.github.io/target/)

> 这里强烈推荐 [HTTP/2.0](https://hpbn.co/http2/) 这篇文章，个人认为是目前看到的资料里介绍的最全面和详细的，`HTTP/2` 的二进制帧，多路复用，请求优先级，流量控制，服务器端推送以及首部压缩等新改进都涉及到了

<!--more-->



## HTTP/2.0 的前世

http2.0的前世是http1.0和http1.1。虽然之前仅仅只有两个版本，但这两个版本所包含的协议规范之庞大，足以让任何一个有经验的工程师为之头疼。http1.0诞生于1996年，协议文档足足60页。之后第三年，http1.1也随之出生，协议文档膨胀到了176页。但是网络协议新版本并不会马上取代旧版本。实际上，1.0和1.1在之后很长的一段时间内一直并存，这是由于网络基础设施更新缓慢所决定的。今天的http2.0也是一样，新版协议再好也需要业界的产品锤炼，需要基础设施逐年累月的升级换代才能普及。

## HTTP站在TCP之上

理解http协议之前一定要对TCP有一定基础的了解。HTTP是建立在TCP协议之上，TCP协议作为传输层协议其实离应用层并不远。HTTP协议的瓶颈及其优化技巧都是基于TCP协议本身的特性。比如TCP建立连接时三次握手有1.5个RTT（round-trip time）的延迟，为了避免每次请求的都经历握手带来的延迟，应用层会选择不同策略的http长链接方案。又比如TCP在建立连接的初期有慢启动（slow start）的特性，所以连接的重用总是比新建连接性能要好。

## HTTP应用场景

http诞生之初主要是应用于web端内容获取，那时候内容还不像现在这样丰富，排版也没那么精美，用户交互的场景几乎没有。对于这种简单的获取网页内容的场景，http表现得还算不错。但随着互联网的发展和web2.0的诞生，更多的内容开始被展示（更多的图片文件），排版变得更精美（更多的css），更复杂的交互也被引入（更多的js）。用户打开一个网站首页所加载的数据总量和请求的个数也在不断增加。今天绝大部分的门户网站首页大小都会超过2M，请求数量可以多达100个。另一个广泛的应用是在移动互联网的客户端app，不同性质的app对http的使用差异很大。对于电商类app，加载首页的请求也可能多达10多个。对于微信这类IM，http请求可能仅限于语音和图片文件的下载，请求出现的频率并不算高。

## 带宽和延迟

影响一个网络请求的因素主要有两个，带宽和延迟。今天的网络基础建设已经使得带宽得到极大的提升，大部分时候都是延迟在影响响应速度。http1.0被抱怨最多的就是连接无法复用，和head of line blocking这两个问题。理解这两个问题有一个十分重要的前提：客户端是依据域名来向服务器建立连接，一般PC端浏览器会针对单个域名的server同时建立6～8个连接，手机端的连接数则一般控制在4～6个。显然连接数并不是越多越好，资源开销和整体延迟都会随之增大。

连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。head of line blocking会导致带宽无法被充分利用，以及后续健康请求被阻塞。假设有5个请求同时发出，如下图

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-01.png)

对于http1.0的实现，在第一个请求没有收到回复之前，后续从应用层发出的请求只能排队，请求2，3，4，5只能等请求1的response回来之后才能逐个发出。网络通畅的时候性能影响不大，一旦请求1的request因为什么原因没有抵达服务器，或者response因为网络阻塞没有及时返回，影响的就是所有后续请求，问题就变得比较严重了。

> 这里可以参考 [https://http2.akamai.com/demo](https://http2.akamai.com/demo) 这个地址，它主要用来比较 `HTTP/2.0` 与 `HTTP/1.1` 在性能上的差异

## 解决连接无法复用

http1.0协议头里可以设置Connection:Keep-Alive。在header里设置Keep-Alive可以在一定时间内复用连接，具体复用时间的长短可以由服务器控制，一般在15s左右。到http1.1之后Connection的默认值就是Keep-Alive，如果要关闭连接复用需要显式的设置Connection:Close。一段时间内的连接复用对PC端浏览器的体验帮助很大，因为大部分的请求在集中在一小段时间以内。但对移动app来说，成效不大，app端的请求比较分散且时间跨度相对较大。所以移动端app一般会从应用层寻求其它解决方案，长连接方案或者伪长连接方案：


#### 基于tcp的长链接

现在越来越多的移动端app都会建立一条自己的长链接通道，通道的实现是基于tcp协议。基于tcp的socket编程技术难度相对复杂很多，而且需要自己制定协议，但带来的回报也很大。信息的上报和推送变得更及时，在请求量爆发的时间点还能减轻服务器压力（http短连接模式会频繁的创建和销毁连接）。不止是IM app有这样的通道，像淘宝这类电商类app都有自己的专属长连接通道了。现在业界也有不少成熟的方案可供选择了，google的protobuf就是其中之一。

#### http long-polling

long-polling可以用下图表示

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-02.png)

客户端在初始状态就会发送一个polling请求到服务器，服务器并不会马上返回业务数据，而是等待有新的业务数据产生的时候再返回。所以连接会一直被保持，一旦结束马上又会发起一个新的polling请求，如此反复，所以一直会有一个连接被保持。服务器有新的内容产生的时候，并不需要等待客户端建立一个新的连接。做法虽然简单，但有些难题需要攻克才能实现稳定可靠的业务框架

* 和传统的http短链接相比，长连接会在用户增长的时候极大的增加服务器压力
* 移动端网络环境复杂，像wifi和4g的网络切换，进电梯导致网络临时断掉等，这些场景都需要考虑怎么重建健康的连接通道。
* 这种polling的方式稳定性并不好，需要做好数据可靠性的保证，比如重发和ack机制。
* polling的response有可能会被中间代理cache住，要处理好业务数据的过期机制。

long-polling方式还有一些缺点是无法克服的，比如每次新的请求都会带上重复的header信息，还有数据通道是单向的，主动权掌握在server这边，客户端有新的业务请求的时候无法及时传送


#### http streaming

http streaming流程大致如下

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-03.png)

同long-polling不同的是，server并不会结束初始的streaming请求，而是持续的通过这个通道返回最新的业务数据。显然这个数据通道也是单向的。streaming是通过在server response的头部里增加”Transfer Encoding: chunked”来告诉客户端后续还会有新的数据到来。除了和long－polling相同的难点之外，streaming还有几个缺陷

* 有些代理服务器会等待服务器的response结束之后才会将结果推送到请求客户端。对于streaming这种永远不会结束的方式来说，客户端就会一直处于等待response的过程中
* 业务数据无法按照请求来做分割，所以客户端没收到一块数据都需要自己做协议解析，也就是说要做自己的协议定制。


#### web socket

WebSocket和传统的tcp socket连接相似，也是基于tcp协议，提供双向的数据通道。WebSocket优势在于提供了message的概念，比基于字节流的tcp socket使用更简单，同时又提供了传统的http所缺少的长连接功能。不过WebSocket相对较新，2010年才起草，并不是所有的浏览器都提供了支持。各大浏览器厂商最新的版本都提供了支持。



## 解决head of line blocking

Head of line blocking(以下简称为holb)是http2.0之前网络体验的最大祸源。正如图1中所示，健康的请求会被不健康的请求影响，而且这种体验的损耗受网络环境影响，出现随机且难以监控。为了解决holb带来的延迟，协议设计者设计了一种新的pipelining机制。

pipelining的流程图可以用下图表示

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-04.png)

和图一相比最大的差别是，请求2，3，4，5不用等请求1的response返回之后才发出，而是几乎在同一时间把request发向了服务器。2，3，4，5及所有后续共用该连接的请求节约了等待的时间，极大的降低了整体延迟。下图可以清晰的看出这种新机制对延迟的改变：

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-05.png)


不过pipelining并不是救世主，它也存在不少缺陷：

* pipelining只能适用于http1.1，一般来说，支持http1.1的server都要求支持pipelining。
* 只有幂等的请求（GET，HEAD）能使用pipelining，非幂等请求比如POST不能使用，因为请求之间可能会存在先后依赖关系。
* head of line blocking并没有完全得到解决，server的response还是要求依次返回，遵循FIFO(first in first out)原则。也就是说如果请求1的response没有回来，2，3，4，5的response也不会被送回来。
* 绝大部分的http代理服务器不支持pipelining。
* 和不支持pipelining的老服务器协商有问题。
* 可能会导致新的Front of queue blocking问题。

正是因为有这么多的问题，各大浏览器厂商要么是根本就不支持pipelining，要么就是默认关掉了pipelining机制，而且启用的条件十分苛刻。可以参考chrome对于pipeling的 [问题描述](https://www.chromium.org/developers/design-documents/network-stack/http-pipelining)


## 开拓者SPDY

http1.0和1.1虽然存在这么多问题，业界也想出了各种优化的手段，但这些方法手段都是在尝试绕开协议本身的缺陷，都有种隔靴搔痒，治标不治本的感觉。直到2012年google如一声惊雷提出了SPDY的方案，大家才开始从正面看待和解决老版本http协议本身的问题，这也直接加速了http2.0的诞生。实际上，http2.0是以SPDY为原型进行讨论和标准化的。为了给http2.0让路，google已决定在2016年不再继续支持SPDY开发，但在http2.0出生之前，SPDY已经有了相当规模的应用，作为一个过渡方案恐怕在还将一段时间内继续存在。现在不少app客户端和server都已经使用了SPDY来提升体验，http2.0在老的设备和系统上还无法使用（iOS系统只有在iOS9+上才支持），所以可以预见未来几年spdy将和http2.0共同服务的情况。

## SPDY的目标

SPDY的目标在一开始就是瞄准http1.x的痛点，即延迟和安全性。我们上面通篇都在讨论延迟，至于安全性，由于http是明文协议，其安全性也一直被业界诟病，不过这是另一个大的话题。如果以降低延迟为目标，应用层的http和传输层的tcp都是都有调整的空间，不过tcp作为更底层协议存在已达数十年之久，其实现已深植全球的网络基础设施当中，如果要动必然伤经动骨，业界响应度必然不高，所以SPDY的手术刀对准的是http。

* 降低延迟，客户端的单连接单请求，server的FIFO响应队列都是延迟的大头。
* http最初设计都是客户端发起请求，然后server响应，server无法主动push内容到客户端。
* 压缩http header，http1.x的header越来越膨胀，cookie和user agent很容易让header的size增至1kb大小，甚至更多。而且由于http的无状态特性，header必须每次request都重复携带，很浪费流量。

为了增加业界响应的可能性，聪明的google一开始就避开了从传输层动手，而且打算利用开源社区的力量以提高扩散的力度，对于协议使用者来说，也只需要在请求的header里设置user agent，然后在server端做好支持即可，极大的降低了部署的难度。SPDY的设计如下：

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-06.png)

SPDY位于HTTP之下，TCP和SSL之上，这样可以轻松兼容老版本的HTTP协议(将http1.x的内容封装成一种新的frame格式)，同时可以使用已有的SSL功能。SPDY的功能可以分为基础功能和高级功能两部分，基础功能默认启用，高级功能需要手动启用。

## SPDY基础功能

* 多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了http1.x holb（head of line blocking）的问题，降低了延迟同时提高了带宽的利用率。
* 请求优先级（request prioritization）。多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。
* header压缩。前面提到过几次http1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。SPDY对header的压缩率可以达到80%以上，低带宽环境下效果很大。

## SPDY高级功能

* server推送（server push）。http1.x只能由客户端发起请求，然后服务器被动的发送response。开启server push之后，server通过X-Associated-Content header（X-开头的header都属于非标准的，自定义header）告知客户端会有新的内容推送过来。在用户第一次打开网站首页的时候，server将资源主动推送过来可以极大的提升用户体验。
* server暗示（server hint）。和server push不同的是，server hint并不会主动推送内容，只是告诉有新的内容产生，内容的下载还是需要客户端主动发起请求。server hint通过X-Subresources header来通知，一般应用场景是客户端需要先查询server状态，然后再下载资源，可以节约一次查询请求。


## SPDY的效果

SPDY的成绩可以用google官方的一个数字来说明：页面加载时间相比于http1.x减少了64%。而且各大浏览器厂商在SPDY诞生之后的1年多里都陆续支持了SPDY，不少大厂app和server端框架也都将SPDY应用到了线上的产品当中。

google的官网也给出了他们自己做的一份测试数据。测试对象是25个访问量排名靠前的网站首页，家用网络%1的丢包率，每个网站测试10次取平均值。结果如下：

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-07.png)

不开启ssl的时候提升在 27% - 60%，开启之后为39% - 55%。 这份测试结果有两点值得特别注意：

* 连接数的选择

连接到底是基于域名来建立，还是不做区分所有子域名都共享一个连接，这个策略选择上值得商榷。google的测试结果测试了两种方案，看结果似乎是单一连接性能高于多域名连接方式。之所以出现这种情况是由于网页所有的资源请求并不是同一时间发出，后续发出的子域名请求如果能复用之前的tcp连接当然性能更好。实际应用场景下应该也是单连接共享模式表现好。

* 带宽的影响

测试基于两种带宽环境，一慢一快。网速快的环境下对减小延迟的提升更大，单连接模式下可以提升至60%。原因也比较简单，带宽越大，复用连接的请求完成越快，由于三次握手和慢启动导致的延迟损耗就变得更明显

除了连接模式和带宽之外，丢包率和RTT也是需要测试的参数。SPDY对header的压缩有80%以上，整体包大小能减少大概40%，发送的包越少，自然受丢包率影响也就越小，所以丢包率大的恶劣环境下SPDY反而更能提升体验。下图是受丢包率影响的测试结果，丢包率超过2.5％之后就没有提升了：

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-08.png)

RTT越大，延迟会越大，在高RTT的场景下，由于SPDY的request是并发进行的，所有对包的利用率更高，反而能更明显的减小总体延迟。测试结果如下：

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-09.png)

SPDY从2012年诞生到2016停止维护，时间跨度对于网络协议来说其实非常之短。但SPDY也完成了自己的使命，也许作为一贯扮演拓荒者角色的google应该也早就预见了这样的结局



## HTTP2.0

SPDY的诞生和表现说明了两件事情：一是在现有互联网设施基础和http协议广泛使用的前提下，是可以通过修改协议层来优化http1.x的。二是针对http1.x的修改确实效果明显而且业界反馈很好。正是这两点让IETF（Internet Enginerring Task Force）开始正式考虑制定HTTP2.0的计划，最后决定以SPDY／3为蓝图起草HTTP2.0，SPDY的部分设计人员也被邀请参与了HTTP2.0的设计

## HTTP2.0需要考虑的问题

* http/0.9 :只支持get方法，不支持多媒体内容的 MIME 类型、各种 HTTP 首部，或者版本号，只是为了获取html对象。
* http/1.0 :添加了版本号、各种 HTTP 首部、一些额外的方法，以及对多媒体对象的处理。
* http/1.0+ :keep-alive 连接、虚拟主机支持，以及代理连接支持都被加入到 HTTP 之中等等。
* http/1.1: 重点关注的是校正 HTTP 设计中的结构性缺陷，明确语义，引入重要 的性能优化措施，并删除一些不好的特性：如Entity tag，If-Unmodified-Since, If-Match, If-None-Match；请求头引入了range头域，它允许只请求资源的某个部分（206）；新增了更多的状态码；Host头处理（400）
* HTTP/2.0 被寄予了如下期望:
  * 相比于使用 TCP 的 HTTP/1.1，最终用户可感知的多数延迟都有能够量化的显 著改善;
  * 解决 HTTP 中的队头阻塞问题;
  * 并行的实现机制不依赖与服务器建立多个连接，从而提升 TCP 连接的利用率，特别是在拥塞控制方面;
  * 保留 HTTP/1.1 的语义，可以利用已有的文档资源(如上所述)，包括(但不限于)HTTP 方法、状态码、URI 和首部字段;
  * 明确定义 HTTP/2.0 和 HTTP/1.x 交互的方法，特别是通过中介时的方法(双向);
  * 明确指出它们可以被合理使用的新的扩展点和策略。




## 连接

连接是所有 HTTP/2 会话的基础元素，其定义是客户端初始化的一个 TCP/IP socket，客户端
是指发送 HTTP 请求的实体。这和 h1 是一样的，不过与完全无状态的 h1 不同的是，h2 把它所承载的帧(frame)和流(stream)共同依赖的连接层元素捆绑在一起，其中既包含连接层设置也包含首部表

> 判断是否支持http/2.0🍏协议发现——识别终端是否支持你想使用的协议——会比较棘手。HTTP/2 提供两种协
议发现的机制。
🍐在连接不加密的情况下，客户端会利用 Upgrade 首部来表明期望使用 h2。如果服务器
也可以支持 h2，它会返回一个“101 Switching Protocols”(协议转换)响应。这增加了
一轮完整的请求-响应通信。🍑如果连接基于 TLS，情况就不同了。客户端在 ClientHello 消息中设置 ALPN
(Application-Layer Protocol Negotiation，应用层协议协商)扩展来表明期望使用 h2 协
议，服务器用同样的方式回复。

为了向服务器双重确认客户端支持 h2，客户端会发送一个叫作 connection preface(连接 前奏)的魔法字节流，作为连接的第一份数据。这主要是为了应对客户端通过纯文本的 HTTP/1.1 升级上来的情况。该字节流用十六进制表示如下:

```js
0x505249202a20485454502f322e300d0a0d0a534d0d0a0d0a
```

解码为 ASCII 是:

```js
PRI * HTTP/2.0\r\n\r\nSM\r\n\r\n 
```

这个字符串的用处是，如果服务器(或者中间网络设备)不支持 h2，就会产生一个显式错误。这个消息特意设计成 h1 消息的样式。如果运行良好的 h1 服务器收到这个字符串，它会阻塞这个方法(PRI)或者版本(HTTP/2.0)，并返回错误，可以让 h2 客户端明确地知道发生了什么错误。 

这个魔法字符串会有一个 SETTINGS 帧紧随其后。服务器为了确认它可以支持 h2，会声明收到客户端的 SETTINGS 帧，并返回一个它自己的 SETTINGS 帧(反过来也需要确认)，
然后确认环境正常，可以开始使用 h2。 (注意⚠️：SETTINGS 帧是个非常重要的帧，http/2.0有十几个帧，不同的帧代表不一样的状态功能)



## 帧

HTTP/2 是基于帧(frame)的协议。采用分帧是为了将重要信息都封装起来，让协议的解析方可以轻松阅读、解析并还原信息。 相比之下，h1 不是基于帧的，而是以 文本分隔。 看看下面的简单例子:

```js
GET / HTTP/1.1 <crlf>
Host: www.example.com <crlf>
Connection: keep-alive <crlf>
Accept: text/html,application/xhtml+xml,application/xml;q=0.9... <crlf>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4)... <crlf>
Accept-Encoding: gzip, deflate, sdch <crlf>
Accept-Language: en-US,en;q=0.8 <crlf>
Cookie: pfy_cbc_lb=p-browse-w; customerZipCode=99912|N; ltc=%20;...<crlf> 
```

解析这种数据用不着什么高科技，但往往速度慢且容易出错。你需要不断读入字节，直到 遇到分隔符为止(这里是指 <crlf>)，同时还要考虑一些不太守规矩的客户端，它们会只 发送 <lf>。

解析 h1 的请求或响应可能出现下列问题：

* 一次只能处理一个请求或响应，完成之前不能停止解析。
* 无法预判解析需要多少内存。这会带来一系列问题:你要把一行读到多大的缓冲区里。

如果行太长会发生什么;应该增加并重新分配内存，还是返回 400 错误。为了解决这些问题，保持内存处理的效率和速度可不简单。 

从另一方面来说，有了帧，处理协议的程序就能预先知道会收到什么。基于帧的协议，特别是 h2，开始有固定长度的字节，其中包含表示整帧长度的字段。

我们看一下帧结构

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-15.png)

Length： 3 字节   表示帧负载的长度(取值范围为 2^ 14~2^24-1 字节)。 请注意，2^14 字节是默认的最大帧大小，如果需要更大的帧，必须在 SETTINGS 帧中设置 。
Type： 1 字节  当前帧类型
Flags：1 字节  具体帧类型的标识
R： 1 位  保留位，不要设置，否则可能带来严重后果
Stream Identifier： 31 位  每个流的唯一 ID 
Frame Payload：长度可变   真实的帧内容，长度是在 Length 字段中设置的 

 因为规范严格明确，所以解析逻辑大概是这样：

```js
loop
   Read 9 bytes off the wire                          //读前9个字节
   Length = the first three bytes                     //长度值为前3字节
   Read the payload based on the length.              //基于长度读负载
   Take the appropriate action based on the frame type. // 根据帧类型采取对应操作
end loop 
```

http/1有个特性叫管道化(pipelining)，允许一次发送一组请求，但是只能按照发送顺序依次接 收响应。而且，管道化备受互操作性和部署的各种问题的困扰，基本没有实用价值。 在请求应答过程中，如果出现任何状况，剩下所有的工作都会被阻塞在那次请求应答之 后。这就是“队头阻塞”.它会阻碍网络传输和 Web 页面渲染，直至失去响应。为了防止 这种问题，现代浏览器会针对单个域名开启 6 个连接，通过各个连接分别发送请求。

> 它实 现了某种程度上的并行，但是每个连接仍会受到“队头阻塞”的影响。由于 h2 是分帧的， 请求和响应可以交错甚至多路复用。多路复用有助于解决类似队头阻塞的问题。 

我们下面来看下队头阻塞的过程

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-16.png)

* 图中第一种请求方式，就是单次发送request请求，收到response后再进行下一次请求，显示是很低效的。
* 于是http1.1提出了管线化(pipelining)技术，就是如图中第二中请求方式，一次性发送多个request请求。
* 然而pipelining在接收response返回时，也必须依顺序接收，如果前一个请求遇到了阻塞，后面的请求即使已经处理完毕了，仍然需要等待阻塞的请求处理完毕。这种情况就如图中第三种，第一个请求阻塞后，后面的请求都需要等待，这也就是队头阻塞(Head of line blocking)。
* 为了解决上述阻塞问题，http2中提出了多路复用(Multiplexing)技术，Multiplexing是通信和计算机网络领域的专业名词。http2中将多个请求复用同一个tcp链接中，将一个TCP连接分为若干个流（Stream），每个流中可以传输若干消息（Message），每个消息由若干最小的二进制帧（Frame）组成。也就是将每个request-response拆分为了细小的二进制帧Frame，这样即使一个请求被阻塞了，也不会影响其他请求，如图中第四种情况所示。


## 流

HTTP/2 规范对流(stream)的定义是:“HTTP/2 连接上独立的、双向的帧序列交换。”你可以将流看作在连接上的一系列帧，它们构成了单独的 HTTP 请求和响应。如果客户端想要发出请求，它会开启一个新的流。然后，服务器将在这个流上回复。这与 h1 的请求 / 响应流程类似，重要的区别在于，因为有分帧，所以多个请求和响应可以交错，而不会互相阻塞。 

一个完整的请求-响应数据交互过程，具有如下几个特点：

双向性：同一个流内，可同时发送和接受数据；
有序性：帧（frames）在流上的发送顺序很重要. 接收方将按照他们的接收顺序处理这些frame. 特别是HEADERS和DATA frame的顺序, 在协议的语义上显得尤为重要.

* 流的创建：流可以被客户端或服务器单方面建立, 使用或共享；
* 流的关闭：流也可以被任意一方关闭
* 多路复用：一个连接同一时刻可以被多个流使用
* 流的并发性：某一时刻，连接上流的并发数。

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-17.png)

强调一下多路复用的好处：

* 减少服务端连接压力，减少占用内存，提升连接吞吐量；
* 连接数的减少改善了网络拥塞状况，慢启动时间减少，拥塞和丢包恢复速度更快；
* 避免连接频繁创建和关闭（三次连接、四次挥手）；

连接、流和帧的关系

* 一个连接同时被多个流复用；
* 一个流代表一次完整的请求/响应过程，包含多个帧；
* 一个消息被拆分封装成多个帧进行传输； 

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-18.png)


## 消息

HTTP 消息泛指 HTTP 请求或响应。 流是用来传输一对请求 / 响 应消息的。一个消息至少由 HEADERS 帧(它初始化流)组成，并且可以另外包含 CONTINUATION 和 DATA 帧，以及其他的 HEADERS 帧。 

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-19.png)

h1 的请求和响应都分成消息首部和消息体两部分;与之类似，h2 的请求和响应分成HEADERS 帧和 DATA 帧。

h1 把消息分成两部分:请求 / 状态行;首部。h2 取消了这种区分，并把这些行变成了 魔法伪首部。举个例子，HTTP/1.1 的请求和响应可能是这样的:

```js
 GET / HTTP/1.1
     Host: www.example.com
     User-agent: Next-Great-h2-browser-1.0.0
     Accept-Encoding: compress, gzip
     HTTP/1.1 200 OK
     Content-type: text/plain
     Content-length: 2     ...
```

在 HTTP/2 中，它等价于:

```js
 :scheme: https:method: GET
 :path: /
 :authority: www.example.com
 User-agent: Next-Great-h2-browser-1.0.0
 Accept-Encoding: compress, gzip 
 :status: 200 
 content-type: text/plain
```

请注意，请求和状态行在这里拆分成了多个首部，即 :scheme、:method、:path 和 :status。 同时要注意的是，http/2.0 的这种表示方式跟数据传输时不同。 


#### 没有分块编码(chunked encoding) 

在基于帧的世界里，谁还需要分块?只有在无法预先知道数据长度的情况下向对方发送 数据时，才会用到分块。在使用帧作为核心协议的 h2 里，就不再需要它了。

#### 不再有101的响应

Switching Protocol 响应是 h1 的边缘应用。它如今最常见的应用可能就是用以升级到 WebSocket 连接。ALPN 提供了更明确的协议协商路径，往返的开销也更小。 



## 流量控制

h2 的新特性之一是基于流的流量控制。不同于 h1 的世界，只要客户端可以处理，服务端就会尽可能快地发送数据，h2 提供了客户端调整传输速度的能力。(并且，由于在 h2 中，
一切几乎都是对称的，服务端也可以调整传输的速度。)WINDOW_UPDATE 帧用来指示流量控制信息。每个帧告诉对方，发送方想要接收多少字节。 

客户端有很多理由使用流量控制。一个很现实的原因可能是，确保某个流不会阻塞其他流。也可能客户端可用的带宽和内存比较有限，强制数据以可处理的分块来加载反而可以提升效率。尽管流量控制不能关闭，把窗口最大值设定为设置 2^31-1 就等效于禁用它，至少对小于 2GB 的文件来说是如此。

另一个需要注意的是中间代理。通常情况下，网络内容通过代理或者 CDN 来传输，也许它们就是传输的起点或终点。由于代理两端的吞吐能力可能不同，有了流量控制，代理的两端就可以密切同步，把代理的压力降到最低。 




## 优先级

流的最后一个重要特性是依赖关系。现代浏览器都经过了精心设计，首先请求网页上最重要的元素，以最优的顺序获取资源，由此来优化页面性能。拿到了 HTML 之后，在渲染页面之前，浏览器通常还需要 CSS 和关键 JavaScript 这样的东西。在没有多路复用的时候，在它可以发出对新对象的请求之前，需要等待前一个响应完成。有了 h2，客户端就可以
一次发出所有资源的请求，服务端也可以立即着手处理这些请求。由此带来的问题是，浏览器失去了在 h1 时代默认的资源请求优先级策略。假设服务器同时接收到了 100 个请求，
也没有标识哪个更重要，那么它将几乎同时发送每个资源，次要元素就会影响到关键元素
的传输。 

h2 通过流的依赖关系来解决这个问题。通过 HEADERS 帧和 PRIORITY 帧，客户端可以明确地和服务端沟通它需要什么，以及它需要这些资源的顺序。这是通过声明依赖关系树 和树里的相对权重实现的。

* 依赖关系为客户端提供了一种能力，通过指明某些对象对另一些对象有依赖，告知服务器这些对象应该优先传输。
* 权重让客户端告诉服务器如何确定具有共同依赖关系的对象的优先级。 

我们来看看这个简单的网站

```js
index.html
– header.jpg
– critical.js
– less_critical.js
– style.css
– ad.js
– photo.jpg 
```

在收到主体 HTML 文件之后，客户端会解析它，并生成依赖树，然后给树里的元素分配权重。这时这棵树可能是这样的:


```js
index.html
  – style.css
    – critical.js
      –  less_critical.js (weight 20)
      –  photo.jpg (weight 8)
      –  header.jpg (weight 8)
      –  ad.js (weight 4) 	
```

在这个依赖树里，客户端表明它最需要的是 style.css，其次是 critical.js。没有这两个文件， 它就不能接着渲染页面。等它收到了 critical.js，就可以给出其余对象的相对权重。权重表示服务一个对象时所需要花费的对应“努力”程度。 





## HTTP2.0主要改动

HTTP2.0作为新版协议，改动细节必然很多，不过对应用开发者和服务提供商来说，影响较大的就几点。

#### 多路复用

多路复用允许同时通过单一的 HTTP/2 连接发起多重的 请求/响应 消息，众所周知 ，在 HTTP/1.1 协议中 「浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制。超过限制数目的请求会被阻塞」。

这也是为何一些站点会有多个静态资源 CDN 域名的原因之一，拿 Twitter 为例，[http://twimg.com](http://twimg.com/)，目的就是变相的解决浏览器针对同一域名的请求限制阻塞问题。而 HTTP/2 的多路复用(Multiplexing) 则允许同时通过单一的 HTTP/2 连接发起多重的 请求/响应 消息

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-12.png)

因此 HTTP/2 可以很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息。并行地在同一个 TCP 连接上双向交换消息。




#### 首部压缩（Header Compression）

HTTP/1.1并不支持 HTTP 首部压缩，为此 SPDY 和 HTTP/2 应运而生， SPDY 使用的是通用的 [DEFLATE](https://zh.wikipedia.org/zh-hans/DEFLATE)  算法，而 HTTP/2 则使用了专门为首部压缩而设计的 [HPACK](http://http2.github.io/http2-spec/compression.html) 算法。

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-13.png)


#### 服务端推送（Server Push）

服务端推送是一种在客户端请求之前发送数据的机制。在 HTTP/2 中，服务器可以对客户端的一个请求发送多个响应。Server Push 让 HTTP1.x 时代使用内嵌资源的优化手段变得没有意义；如果一个请求是由你的主页发起的，服务器很可能会响应主页内容、logo 以及样式表，因为它知道客户端会用到这些东西。这相当于在一个 HTML 文档内集合了所有的资源，不过与之相比，服务器推送还有一个很大的优势：可以缓存！也让在遵循同源的情况下，不同页面之间可以共享缓存资源成为可能。

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-14.png)

关于 Server Push ，一个常见的问题是,如果客户端早已在缓存中有了一份 copy 怎么办？

因为 Push 本身具有投机性，所以肯定会出现推送过去的东西浏览器不需要的情况。这种情况下，HTTP/2 允许客户端通过 RESET_STREAM 主动取消 Push ，然而这样的话，原本可以用于更好方向的 Push 就白白的浪费掉数据往返的价值。

对此，一个推荐的解决方案是，客户端使用一个简洁的 [Cache Digest](https://tools.ietf.org/html/draft-ietf-httpbis-cache-digest-05) 来告诉服务器，哪些东西已经在缓存，因此服务器也就会知道哪些是客户端所需要的。

因为 Cache Digest 使用的是 [Golumb Compressed Sets](https://www.imperialviolet.org/2011/04/29/filters.html)，浏览器客户端可以通过一个连接发送少于 1K 字节的 Packets 给服务端，通知哪些是已经在缓存中存在的

现在，我们已经解决了 chattiness、额外的 Round Trip 开销、啰嗦首部的数据浪费、inline 以及过去的其他优化行为、最后则是 Push 重复资源带来数据浪费，这和我们理想化的目标越来越近了。

Cache Digests 只是其中一个提案之一， 在 HTTP 社区有着更多其他的解决方案，我们也希望在不久的将来看到他们的身影。






## http/2.0反模式

h1 下的一些性能调优办法在 h2 下会起到反作用


#### 域名拆分

域名拆分是为了利用浏览器对每个域名开启多个连接的能力，以便实现资源的并行下载，
绕过 h1 的串行化下载的限制。对于包含大量小型资源的网站，普遍的做法是拆分域名，
以利用现代浏览器针能对每个域名开启 6 个连接的特性。这样实际上做到了让浏览器并行
发送多个请求，以及充分利用可用带宽的效果。因为 HTTP/2 采取多路复用，所以域名拆
分就不是必要的了，并且反而会让协议力图实现的目标落空。 



#### 资源内联

资源内联包括把 JavaScript、样式，甚至图片插入到 HTML 页面中，目的是省掉加载外部资源所需的新连接以及请求响应的时间。然而，有些 Web 性能的最佳实践不推荐使用内联，因为这样会损失更有价值的特性，比如缓存。如果有同一个页面上的重复访问，缓存通常可以减少请求数(而且能够加速页面渲染)。尽管如此，总体来说，对那些渲染滚动
条以上区域所需的微小资源进行内联处理仍是值得的。

> 事实上有证据表明，在性能较弱的 设备上，缓存对象的好处不够多，把内联资源拆分出来并不划算。使用 h2 时的一般原则是避免内联，但是内联也并不一定毫无价值。 


#### 资源合并

资源合并意味着把几个小文件合并成一个大文件。它与内联很相似，旨在省掉那些加载外部资源的请求响应时间，以及解码 / 执行那些资源所消耗的 CPU 资源。之前针对资源内联
的规则同样适用于资源合并，我们可以使用它来合并非常小的文件(1KB 或更小)，以及
对初始渲染很关键的最简化 JavaScript/CSS 资源。 


#### 禁用cookie的域名
			
通过禁用 cookie 的域名来提供静态资源是一项标准的性能优化最佳实践。尤其是使用 h1
时，你无法压缩首部，而且有些网站使用的 cookie 大小常常超过单个 TCP 数据包的限度。
不过，在 h2 下请求首部使用 HPACK 算法被压缩，会显著减少巨型 cookie(尤其是当它
们在先后请求之间保持不变)的字节数。与此同时，禁用 cookie 的域名需要额外的主机名
称，这意味着将开启更多的连接。

> 如果你正在使用禁用 cookie 的域名，以后有机会你可能得考虑消灭它。如果你确实不需要那些域名，最好删掉它们。省一个字节就是一个字节。 
				
			
#### 生成精灵图
			
目前，生成精灵图仍是一种避免小资源请求过多的技术(你能看到人们乐意做什么来优化
h1)。为了生成精灵图，开发者把较小的图片拼合成较大的图片，然后用 CSS 选择图片中
某个部分展示出来。依据设备及其硬件图形处理能力的不同，精灵图要么非常高效，要么
非常低效。如果用 h2，最佳实践就是避免生成精灵图;主要原因在于，多路复用和首部压缩去掉了大量的请求开销。即便如此，还是有些场景适合使用精灵图。


## 性能优化因人而异 
					
为了最大化 Web 性能，需要在许多变量之间取舍，包括网络条件、设备
处理能力、浏览器能力，还有协议限制。这些组成了我们所说的场景，大多数开发者
的时间远远不够考虑那么多场景。 
					
怎么办?最佳实践的第一原则就是:测试。性能测试与监控是获得最大成果的关键，
HTTP/2 也不例外。观察真实用户数据、详尽分析各种条件、查找问题，然后解决它们。要遵循业界推荐的方式，但也不要陷入过早优化的陷阱。




## 参考

* [HTTP/2](https://hpbn.co/http2/)
* [HTTP/2 for Front-End Developers](https://www.mnot.net/talks/h2fe/)
* [HTTP 2.0 协议详解](https://blog.csdn.net/zqjflash/article/details/50179235)
* [Web 性能权威指南](https://item.jd.com/11444582.html)
* [使用 HTTP/2 提升性能的 7 个建议](https://www.w3ctech.com/topic/1563#tip7sharding)
* [HTTP/2 资料汇总](https://www.zhihu.com/question/34074946)