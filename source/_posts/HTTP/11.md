---
title: HTTP/2.0
date: 2020-09-06
categories: HTTP
tags: HTTP
toc: true
thumbnail: https://gitee.com/heptaluan/backups/raw/master/cdn/cover/11.jpg
---

最近在梳理 `HTTP` 协议相关内容，当梳理到追加协议相关内容的时候发现 `HTTP/2.0` 所涉及到的内容篇幅还是有些多的，所以就单独提取出来，更多相关内容可以参考 [HTTP 协议梳理](http://localhost:4000/2020/09/01/HTTP/10/) 和 [前端知识体系整理](https://heptaluan.github.io/target/)

> 这里推荐 [HTTP/2.0](https://hpbn.co/http2/) 这篇文章，个人认为是目前看到的资料里介绍的最全面和详细的，`HTTP/2.0` 的二进制帧，多路复用，请求优先级，流量控制，服务器端推送以及首部压缩等新改进都涉及到了，还可以感受一下 [https://http2.akamai.com/demo](https://http2.akamai.com/demo) 这个地址，它主要用来比较 `HTTP/2.0` 与 `HTTP/1.1` 在性能上的差异

<!--more-->





## HTTP/2.0 的前身

`HTTP/2.0` 的前身是 `HTTP/1.0` 和 `HTTP/1.1`，虽然之前仅仅只有两个版本，但这两个版本所包含的协议规范之庞大，足以让任何一个有经验的工程师为之头疼，`HTTP/1.0` 诞生于 `1996` 年，协议文档足足 `60` 页，之后第三年，`HTTP/1.1` 也随之出生，协议文档膨胀到了 `176` 页，但是网络协议新版本并不会马上取代旧版本，实际上 `HTTP/1.0` 和 `HTTP/1.1` 在之后很长的一段时间内一直并存，这是由于网络基础设施更新缓慢所决定的，今天的 `HTTP/2.0` 也是一样，新版协议再好也需要业界的产品锤炼，需要基础设施逐年累月的升级换代才能普及




## HTTP 站在 TCP 之上

理解 `HTTP` 协议之前一定要对 `TCP` 有一定的了解，`HTTP` 是建立在 `TCP` 协议之上，`TCP` 协议作为传输层协议其实离应用层并不远，`HTTP` 协议的瓶颈及其优化技巧都是基于 `TCP` 协议本身的特性，比如 `TCP` 建立连接时三次握手有 `1.5` 个 `RTT`（来回时间）的延迟，为了避免每次请求的都经历握手带来的延迟，应用层会选择不同策略的 `HTTP` 长链接方案，又比如 `TCP` 在建立连接的初期有慢启动（`slow start`）的特性，所以连接的重用总是比新建连接性能要好




## HTTP 应用场景

`HTTP` 诞生之初主要是应用于 `Web` 端内容获取，对于早些时期简单的获取网页内容的场景，`HTTP` 表现得还算不错，但随着互联网的发展和 `Web 2.0` 的诞生，更多的内容开始被展示（比如更多的的图片文件，`CSS`，`JavaScript` 等），用户打开一个网站首页所加载的数据总量和请求的个数也在不断增加，今天绝大部分的门户网站首页大小都会超过 `2M`，请求数量可以多达 `100` 个，另一个广泛的应用场景就是在移动端，比如对于电商类 `App`，加载首页的请求也可能多达几十个



## 带宽和延迟

影响一个网络请求的因素主要有两个，带宽和延迟，今天的网络基础建设已经使得带宽得到极大的提升，大部分时候都是延迟在影响响应速度，`HTTP/1.0` 被抱怨最多的就是连接无法复用，和队头阻塞这两个问题，理解这两个问题有一个十分重要的前提就是客户端是依据域名来向服务器建立连接，一般浏览器会针对单个域名的服务器同时建立六到八个连接，而移动端的连接数则一般控制在四到六个，显然连接数并不是越多越好，资源开销和整体延迟都会随之增大

连接无法复用会导致每次请求都经历三次握手和慢启动，三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大，队头阻塞会导致带宽无法被充分利用，以及后续健康请求被阻塞，假设有五个请求同时发出，如下图

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-01.png)

对于 `HTTP/1.1` 的实现，在第一个请求没有收到回复之前，后续从应用层发出的请求只能排队，请求 `2，3，4，5` 只能等待请求 `1` 的响应回来之后才能逐个发出，网络通畅的时候性能影响不大，一旦请求 `1` 因为什么原因没有抵达服务器，或者响应因为网络阻塞没有及时返回，影响的就是所有后续请求



## 解决连接无法复用

`HTTP/1.0` 协议头里可以设置 `Connection: Keep-Alive`，在 `Header` 里设置 `Keep-Alive` 可以在一定时间内复用连接，具体复用时间的长短可以由服务器控制，一般在 `15s` 左右，到 `HTTP/1.1` 之后，`Connection` 的默认值就是 `Keep-Alive`，如果要关闭连接复用需要显式的设置 `Connection: Close`，一段时间内的连接复用对电脑端的浏览器体验帮助很大，因为大部分的请求在集中在一小段时间以内，但对与移动端来说，成效不大，因为移动端的请求比较分散且时间跨度相对较大，所以移动端一般会从应用层寻求其它解决方案，比如长连接方案或者伪长连接方案


#### 基于 TCP 的长链接

这种方式是建立一条自己的长链接通道，通道的实现是基于 `TCP` 协议，基于 `TCP` 的 `Socket` 编程技术难度相对复杂很多，而且需要自己制定协议，但带来的回报也很大，信息的上报和推送变得更及时，在请求量爆发的时间点还能减轻服务器压力（`HTTP` 短连接模式会频繁的创建和销毁连接）


#### 长轮询

`HTTP` 长轮询可以用下图表示

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-02.png)

客户端在初始状态就会发送一个轮询请求到服务器，服务器并不会马上返回业务数据，而是等待有新的业务数据产生的时候再返回，所以连接会一直被保持，一旦结束马上又会发起一个新的轮询请求，如此反复，所以一直会有一个连接被保持，服务器有新的内容产生的时候，并不需要等待客户端建立一个新的连接，使用长轮询的好处以及需要注意的地方有以下这些

* 和传统的 `HTTP` 短链接相比，长连接会在用户增长的时候极大的增加服务器压力
* 移动端网络环境复杂，像 `Wi-Fi` 和 `4G` 之类的网络切换，进电梯导致网络临时断掉等，这些场景都需要考虑怎么重建健康的连接通道
* 这种轮询的方式稳定性并不好，需要做好数据可靠性的保证，比如重发和 `ACk` 机制（消息确认机制）
* 轮询的响应有可能会被中间代理缓存，要处理好业务数据的过期机制

长轮询方式还有一些缺点是无法克服的，比如每次新的请求都会带上重复的 `Header` 信息，还有数据通道是单向的，主动权掌握在服务端这边，客户端有新的业务请求的时候无法及时传送


#### 流

`HTTP` 流的过程大致是下面这样的

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-03.png)

同长轮询不同的是，服务端并不会结束初始的流请求，而是持续的通过这个通道返回最新的业务数据，显然这个数据通道也是单向的，流是通过在服务端响应的头部里增加 `Transfer Encoding: chunked` 来告诉客户端后续还会有新的数据到来，除了和长轮询相同的难点之外，流还有几个缺陷

* 有些代理服务器会等待服务器的响应结束之后才会将结果推送到请求客户端，对于流这种永远不会结束的方式来说，客户端就会一直处于等待响应的过程中
* 业务数据无法按照请求来做分割，所以客户端没收到一块数据都需要自己做协议解析，也就是说要做自己的协议定制

> 关于 `HTTP` 流的更为详细的内容，我们会在下面的 `HTTP/2.0` 章节当中进行介绍


#### WebSocket

`WebSocket` 和传统的 `TCP Socket` 连接相似，也是基于 `TCP` 协议，提供双向的数据通道，比基于字节流的 `TCP Socket` 使用更简单，同时又提供了传统的 `HTTP` 所缺少的长连接功能




## 解决队头阻塞

队头阻塞是 `HTTP/2.0` 之前网络体验的最大祸源，正如上面带宽和延迟章节当中的图片所示，健康的请求会被不健康的请求影响，而且这种体验的损耗受网络环境影响，出现随机且难以监控，为了解决队头阻塞带来的延迟，协议设计者设计了一种新的 `HTTP` 管线化机制（`pipelining`），管线化的流程图可以用下图表示

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-04.png)

和之前相比最大的差别是，请求 `2，3，4，5` 不需要等待请求 `1` 的响应返回之后才发出，而是几乎在同一时间就把请求发向了服务器，`2，3，4，5` 及所有后续共用该连接的请求节约了等待的时间，极大的降低了整体延迟，下图可以清晰的看出这种新机制对延迟的改变

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-05.png)

不过管线化并不是完美的，它也存在不少缺陷

* 管线化只能适用于 `HTTP/1.1` ，一般来说，支持 `HTTP/1.1` 的服务端都要求支持管线化
* 只有幂等的请求（`GET`，`HEAD`）能使用管线化，非幂等请求比如 `POST` 不能使用，因为请求之间可能会存在先后依赖关系
* 队头阻塞并没有完全得到解决，服务端的响应还是要求依次返回，遵循 `FIFO`（`first in first out`）原则，也就是说如果请求 `1` 的响应没有回来，`2，3，4，5` 的响应也不会被送回来
* 绝大部分的 `HTTP` 代理服务器不支持管线化

正是因为有这么多的问题，各大浏览器厂商要么是根本就不支持管线化，要么就是默认关掉了管线化机制，而且启用的条件十分苛刻，可以参考 `Chrome` 对于管线化的 [问题描述](https://www.chromium.org/developers/design-documents/network-stack/http-pipelining)



## 开拓者 SPDY

`HTTP/1.0` 和 `HTTP/1.1` 虽然存在这么多问题，业界也想出了各种优化的手段，但这些方法手段都是在尝试绕开协议本身的缺陷，直到 `2012` 年 `Google` 如提出了 `SPDY` 的方案，大家才开始从正面看待和解决老版本 `HTTP` 协议本身的问题，这也直接加速了 `HTTP/2.0` 的诞生，实际上 `HTTP/2.0` 是以 `SPDY` 为原型进行讨论和标准化的

## SPDY 的目标

`SPDY` 的目标在一开始就是瞄准老版本所存在的一些痛点，即延迟和安全性，关于延迟我们在上面已经介绍过了，至于安全性，由于 `HTTP` 是明文协议，其安全性也一直被业界诟病，如果以降低延迟为目标，应用层的 `HTTP` 和传输层的 `TCP` 都是都有调整的空间，不过 `TCP` 作为更底层协议存在已达数十年之久，其实现已深植全球的网络基础设施当中，所以 `SPDY` 的瞄准的就是 `HTTP`，主要有以下几点

* 降低延迟，客户端的单连接单请求，服务端的 `FIFO` 响应队列都是延迟的大头
* `HTTP` 最初设计都是客户端发起请求，然后服务端响应，所以服务端无法主动推送内容到客户端
* 头部压缩，`Cookie` 和 `User-Agent` 很容易让 ` `Header` ` 的大小增加，而且由于 `HTTP` 的无状态特性，`Header` 必须每次请求都重复携带，很浪费流量

为了增加业界响应的可能性，聪明的 `Google` 一开始就避开了从传输层动手，对于协议使用者来说，也只需要在请求的 `Header` 里设置 `User-Agent`，然后在服务端端做好支持即可，极大的降低了部署的难度， `SPDY` 的设计如下

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-06.png)

`SPDY` 位于 `HTTP` 之下，`TCP` 和 `SSL` 之上，这样可以轻松兼容老版本的 `HTTP` 协议，同时可以使用已有的 `SSL` 功能，`SPDY` 的功能可以分为基础功能和高级功能两部分，基础功能默认启用，高级功能需要手动启用，它们有以下这些

* 基础功能
  * 多路复用（`multiplexing`），多路复用通过多个请求流共享一个 `TCP` 连接的方式，解决了队头阻塞的问题，降低了延迟同时提高了带宽的利用率
  * 请求优先级（`request prioritization`），多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞，`SPDY` 允许给每个请求设置优先级，这样重要的请求就会优先得到响应，比如浏览器加载首页，首页的 `HTML` 内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容
  * `Header` 压缩，选择合适的压缩算法可以减小包的大小和数量，`SPDY` 对 `Header` 的压缩率可以达到 `80%` 以上，低带宽环境下效果很大
* 高级功能
  * 服务端推送，最初只能由客户端发起请求，然后服务器被动的发送响应，开启服务端推送之后，服务端通过在 `Header` 中添加 `X-Associated-Content` 头域（`X` 开头的 `Header` 都属于非标准的，自定义 `Header` ）告知客户端会有新的内容推送过来，在用户第一次打开网站首页的时候，服务端将资源主动推送过来可以极大的提升用户体验
  * 服务端暗示，和服务端推送不同的是，服务端暗示并不会主动推送内容，只是告诉有新的内容产生，内容的下载还是需要客户端主动发起请求，服务端暗示通过在 `Header` 中添加 `X-Subresources` 头域来通知，一般应用场景是客户端需要先查询服务端状态，然后再下载资源，可以节约一次查询请求


## SPDY 的效果

`SPDY` 的成绩可以用 `Google` 官方的一个数字来说明，页面加载时间减少了 `64%`，`Google` 的官网也给出了他们自己做的一份测试数据，测试对象是 `25` 个访问量排名靠前的网站首页，家用网络 `%1` 的丢包率，每个网站测试 `10` 次取平均值，结果如下

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-07.png)

不开启 `SSL` 的时候提升在 `27% ~ 60%`，开启之后为 `39% ~ 55%`， 这份测试结果有两点值得特别注意

* 连接数的选择，连接到底是基于域名来建立，还是不做区分所有子域名都共享一个连接，这个策略选择上值得商榷，`Google` 的测试结果测试了两种方案，看结果似乎是单一连接性能高于多域名连接方式，之所以出现这种情况是由于网页所有的资源请求并不是同一时间发出，后续发出的子域名请求如果能复用之前的 `TCP` 连接当然性能更好，实际应用场景下应该也是单连接共享模式表现好
* 带宽的影响，测试基于两种带宽环境，一慢一快，网速快的环境下对减小延迟的提升更大，单连接模式下可以提升至 `60%`，原因也比较简单，带宽越大，复用连接的请求完成越快，由于三次握手和慢启动导致的延迟损耗就变得更明显

除了连接模式和带宽之外，丢包率和 `RTT` 也是需要测试的参数，`SPDY` 对 `Header` 的压缩有 `80%` 以上，整体包大小能减少大概 `40%`，发送的包越少，自然受丢包率影响也就越小，所以丢包率大的恶劣环境下 `SPDY` 反而更能提升体验，下图是受丢包率影响的测试结果，丢包率超过 `2.5%` 之后就没有提升了

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-08.png)

`RTT` 越大，延迟会越大，在高 `RTT` 的场景下，由于 `SPDY` 的请求是并发进行的，所有对包的利用率更高，反而能更明显的减小总体延迟，测试结果如下

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-09.png)

不过 `SPDY` 从 `2012` 年诞生到 `2016` 停止维护，只有短短的四年，时间跨度对于网络协议来说其实非常之短，但 `SPDY` 也完成了自己的使命，也许作为一贯扮演拓荒者角色的 `Google` 应该也早就预见了这样的结局




## HTTP/2.0

`SPDY` 的诞生和表现说明了两件事情，一是在现有互联网设施基础和 `HTTP` 协议广泛使用的前提下，是可以通过修改协议层来优化的，二是修改确实效果明显而且业界反馈很好，正是这两点让 `IETF`（`Internet Enginerring Task Force`）开始正式考虑制定 `HTTP/2.0` 的计划，最后决定以 `SPDY/3` 为蓝图起草 `HTTP/2.0`，`SPDY` 的部分设计人员也被邀请参与了 `HTTP/2.0` 的设计

## HTTP/2.0 需要考虑的问题

`HTTP/1.0` 最早在网页中使用是在 `1996` 年，那个时候只是使用一些较为简单的网页上和网络请求上，而 `HTTP/1.1` 则在 `1999` 年才开始广泛应用于现在的各大浏览器网络请求中，同时 `HTTP/1.1` 也是当前使用最为广泛的HTTP协议

* `HTTP/0.9`，只支持 `GET` 方法，不支持多媒体内容的 `MIME` 类型、各种 `HTTP` 首部，或者版本号，只是为了获取 `HTML` 对象
* `HTTP/1.0`，添加了版本号、各种 `HTTP` 首部、一些额外的方法，以及对多媒体对象的处理
* `HTTP/1.0+`，`Keep-Alive` 连接、虚拟主机支持，以及代理连接支持都被加入到 `HTTP` 之中等等
* `HTTP/1.1`，主要有下面几个部分
  * 缓存处理，在 `HTTP/1.0` 中主要使用 `Header` 里的 `If-Modified-Since`，`Expires` 来做为缓存判断的标准，`HTTP/1.1` 则引入了更多的缓存控制策略例如 `Entity tag`，`If-Unmodified-Since`，`If-Match`，`If-None-Match` 等更多可供选择的缓存头来控制缓存策略
  * 在请求头引入了 `Range` 头域，它允许只请求资源的某个部分，即返回码是 `206`（`Partial Content`）
  * 错误通知的管理，新增了 `24` 个错误状态响应码，如 `409`（`Conflict`）表示请求的资源与资源的当前状态发生冲突，`410`（`Gone`）表示服务器上的某个资源被永久性的删除等
  * `Host` 头处理，`HTTP/1.1` 的请求消息和响应消息都应支持 `Host` 头域，且请求消息中如果没有 `Host` 头域会报告一个错误（`400 Bad Request`）
  * 长连接，`HTTP/1.1` 支持长连接和管线化处理，也就是 `Connection: keep-alive`

而 `HTTP/2.0` 在此基础之上被寄予了如下期望，希望能够

* 相比于使用 `TCP` 的 `HTTP/1.1`，最终用户可感知的多数延迟都有能够量化的显著改善
* 解决 `HTTP` 中的队头阻塞问题
* 并行的实现机制不依赖与服务器建立多个连接，从而提升 `TCP` 连接的利用率，特别是在拥塞控制方面
* 保留 `HTTP/1.1` 的语义，可以利用已有的文档资源，包括 `HTTP` 方法、状态码、`URI` 和首部字段等



## 连接

连接是所有 HTTP/2 会话的基础元素，其定义是客户端初始化的一个 TCP/IP socket，客户端
是指发送 HTTP 请求的实体。这和 h1 是一样的，不过与完全无状态的 h1 不同的是，h2 把它所承载的帧(frame)和流(stream)共同依赖的连接层元素捆绑在一起，其中既包含连接层设置也包含首部表

> 判断是否支持http/2.0🍏协议发现——识别终端是否支持你想使用的协议——会比较棘手。HTTP/2 提供两种协
议发现的机制。
🍐在连接不加密的情况下，客户端会利用 Upgrade 首部来表明期望使用 h2。如果服务器
也可以支持 h2，它会返回一个“101 Switching Protocols”(协议转换)响应。这增加了
一轮完整的请求-响应通信。🍑如果连接基于 TLS，情况就不同了。客户端在 ClientHello 消息中设置 ALPN
(Application-Layer Protocol Negotiation，应用层协议协商)扩展来表明期望使用 h2 协
议，服务器用同样的方式回复。

为了向服务器双重确认客户端支持 h2，客户端会发送一个叫作 connection preface(连接 前奏)的魔法字节流，作为连接的第一份数据。这主要是为了应对客户端通过纯文本的 HTTP/1.1 升级上来的情况。该字节流用十六进制表示如下:

```js
0x505249202a20485454502f322e300d0a0d0a534d0d0a0d0a
```

解码为 ASCII 是:

```js
PRI * HTTP/2.0\r\n\r\nSM\r\n\r\n 
```

这个字符串的用处是，如果服务器(或者中间网络设备)不支持 h2，就会产生一个显式错误。这个消息特意设计成 h1 消息的样式。如果运行良好的 h1 服务器收到这个字符串，它会阻塞这个方法(PRI)或者版本(HTTP/2.0)，并返回错误，可以让 h2 客户端明确地知道发生了什么错误。 

这个魔法字符串会有一个 SETTINGS 帧紧随其后。服务器为了确认它可以支持 h2，会声明收到客户端的 SETTINGS 帧，并返回一个它自己的 SETTINGS 帧(反过来也需要确认)，
然后确认环境正常，可以开始使用 h2。 (注意⚠️：SETTINGS 帧是个非常重要的帧，http/2.0有十几个帧，不同的帧代表不一样的状态功能)



## 帧

HTTP/2 是基于帧(frame)的协议。采用分帧是为了将重要信息都封装起来，让协议的解析方可以轻松阅读、解析并还原信息。 相比之下，h1 不是基于帧的，而是以 文本分隔。 看看下面的简单例子:

```js
GET / HTTP/1.1 <crlf>
Host: www.example.com <crlf>
Connection: keep-alive <crlf>
Accept: text/html,application/xhtml+xml,application/xml;q=0.9... <crlf>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4)... <crlf>
Accept-Encoding: gzip, deflate, sdch <crlf>
Accept-Language: en-US,en;q=0.8 <crlf>
Cookie: pfy_cbc_lb=p-browse-w; customerZipCode=99912|N; ltc=%20;...<crlf> 
```

解析这种数据用不着什么高科技，但往往速度慢且容易出错。你需要不断读入字节，直到 遇到分隔符为止(这里是指 <crlf>)，同时还要考虑一些不太守规矩的客户端，它们会只 发送 <lf>。

解析 h1 的请求或响应可能出现下列问题：

* 一次只能处理一个请求或响应，完成之前不能停止解析。
* 无法预判解析需要多少内存。这会带来一系列问题:你要把一行读到多大的缓冲区里。

如果行太长会发生什么;应该增加并重新分配内存，还是返回 400 错误。为了解决这些问题，保持内存处理的效率和速度可不简单。 

从另一方面来说，有了帧，处理协议的程序就能预先知道会收到什么。基于帧的协议，特别是 h2，开始有固定长度的字节，其中包含表示整帧长度的字段。

我们看一下帧结构

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-15.png)

Length： 3 字节   表示帧负载的长度(取值范围为 2^ 14~2^24-1 字节)。 请注意，2^14 字节是默认的最大帧大小，如果需要更大的帧，必须在 SETTINGS 帧中设置 。
Type： 1 字节  当前帧类型
Flags：1 字节  具体帧类型的标识
R： 1 位  保留位，不要设置，否则可能带来严重后果
Stream Identifier： 31 位  每个流的唯一 ID 
Frame Payload：长度可变   真实的帧内容，长度是在 Length 字段中设置的 

 因为规范严格明确，所以解析逻辑大概是这样：

```js
loop
   Read 9 bytes off the wire                          //读前9个字节
   Length = the first three bytes                     //长度值为前3字节
   Read the payload based on the length.              //基于长度读负载
   Take the appropriate action based on the frame type. // 根据帧类型采取对应操作
end loop 
```

http/1有个特性叫管道化(pipelining)，允许一次发送一组请求，但是只能按照发送顺序依次接 收响应。而且，管道化备受互操作性和部署的各种问题的困扰，基本没有实用价值。 在请求应答过程中，如果出现任何状况，剩下所有的工作都会被阻塞在那次请求应答之 后。这就是“队头阻塞”.它会阻碍网络传输和 Web 页面渲染，直至失去响应。为了防止 这种问题，现代浏览器会针对单个域名开启 6 个连接，通过各个连接分别发送请求。

> 它实 现了某种程度上的并行，但是每个连接仍会受到“队头阻塞”的影响。由于 h2 是分帧的， 请求和响应可以交错甚至多路复用。多路复用有助于解决类似队头阻塞的问题。 

我们下面来看下队头阻塞的过程

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-16.png)

* 图中第一种请求方式，就是单次发送request请求，收到响应后再进行下一次请求，显示是很低效的。
* 于是http1.1提出了管线化(pipelining)技术，就是如图中第二中请求方式，一次性发送多个request请求。
* 然而pipelining在接收响应返回时，也必须依顺序接收，如果前一个请求遇到了阻塞，后面的请求即使已经处理完毕了，仍然需要等待阻塞的请求处理完毕。这种情况就如图中第三种，第一个请求阻塞后，后面的请求都需要等待，这也就是队头阻塞(Head of line blocking)。
* 为了解决上述阻塞问题，http2中提出了多路复用(Multiplexing)技术，Multiplexing是通信和计算机网络领域的专业名词。http2中将多个请求复用同一个tcp链接中，将一个TCP连接分为若干个流（Stream），每个流中可以传输若干消息（Message），每个消息由若干最小的二进制帧（Frame）组成。也就是将每个request-响应拆分为了细小的二进制帧Frame，这样即使一个请求被阻塞了，也不会影响其他请求，如图中第四种情况所示。

下面我们就稍微的深入的来了解一下 HTTP/2 当中帧的概念，主要涉及到帧大小，帧格式和帧定义


## 流

HTTP/2 规范对流(stream)的定义是:“HTTP/2 连接上独立的、双向的帧序列交换。”你可以将流看作在连接上的一系列帧，它们构成了单独的 HTTP 请求和响应。如果客户端想要发出请求，它会开启一个新的流。然后，服务器将在这个流上回复。这与 h1 的请求 / 响应流程类似，重要的区别在于，因为有分帧，所以多个请求和响应可以交错，而不会互相阻塞。 

一个完整的请求-响应数据交互过程，具有如下几个特点：

双向性：同一个流内，可同时发送和接受数据；
有序性：帧（frames）在流上的发送顺序很重要. 接收方将按照他们的接收顺序处理这些frame. 特别是HEADERS和DATA frame的顺序, 在协议的语义上显得尤为重要.

* 流的创建：流可以被客户端或服务器单方面建立, 使用或共享；
* 流的关闭：流也可以被任意一方关闭
* 多路复用：一个连接同一时刻可以被多个流使用
* 流的并发性：某一时刻，连接上流的并发数。

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-17.png)



这里我们简单的总结一下连接、流和帧的关系

* 一个连接同时被多个流复用；
* 一个流代表一次完整的请求/响应过程，包含多个帧；
* 一个消息被拆分封装成多个帧进行传输； 

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-18.png)


## 消息

HTTP 消息泛指 HTTP 请求或响应。 流是用来传输一对请求 / 响 应消息的。一个消息至少由 HEADERS 帧(它初始化流)组成，并且可以另外包含 CONTINUATION 和 DATA 帧，以及其他的 HEADERS 帧。 

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-19.png)

h1 的请求和响应都分成消息首部和消息体两部分;与之类似，h2 的请求和响应分成HEADERS 帧和 DATA 帧。

h1 把消息分成两部分:请求 / 状态行;首部。h2 取消了这种区分，并把这些行变成了 魔法伪首部。举个例子，HTTP/1.1 的请求和响应可能是这样的:

```js
 GET / HTTP/1.1
     Host: www.example.com
     User-agent: Next-Great-h2-browser-1.0.0
     Accept-Encoding: compress, gzip
     HTTP/1.1 200 OK
     Content-type: text/plain
     Content-length: 2     ...
```

在 HTTP/2 中，它等价于:

```js
 :scheme: https:method: GET
 :path: /
 :authority: www.example.com
 User-agent: Next-Great-h2-browser-1.0.0
 Accept-Encoding: compress, gzip 
 :status: 200 
 content-type: text/plain
```

请注意，请求和状态行在这里拆分成了多个首部，即 :scheme、:method、:path 和 :status。 同时要注意的是，http/2.0 的这种表示方式跟数据传输时不同。 


#### 没有分块编码(chunked encoding) 

在基于帧的世界里，谁还需要分块?只有在无法预先知道数据长度的情况下向对方发送 数据时，才会用到分块。在使用帧作为核心协议的 h2 里，就不再需要它了。

#### 不再有101的响应

Switching Protocol 响应是 h1 的边缘应用。它如今最常见的应用可能就是用以升级到 WebSocket 连接。ALPN 提供了更明确的协议协商路径，往返的开销也更小。 



## 流量控制

h2 的新特性之一是基于流的流量控制。不同于 h1 的世界，只要客户端可以处理，服务端就会尽可能快地发送数据，h2 提供了客户端调整传输速度的能力。(并且，由于在 h2 中，
一切几乎都是对称的，服务端也可以调整传输的速度。)WINDOW_UPDATE 帧用来指示流量控制信息。每个帧告诉对方，发送方想要接收多少字节。 

客户端有很多理由使用流量控制。一个很现实的原因可能是，确保某个流不会阻塞其他流。也可能客户端可用的带宽和内存比较有限，强制数据以可处理的分块来加载反而可以提升效率。尽管流量控制不能关闭，把窗口最大值设定为设置 2^31-1 就等效于禁用它，至少对小于 2GB 的文件来说是如此。

另一个需要注意的是中间代理。通常情况下，网络内容通过代理或者 CDN 来传输，也许它们就是传输的起点或终点。由于代理两端的吞吐能力可能不同，有了流量控制，代理的两端就可以密切同步，把代理的压力降到最低。 




## 优先级

流的最后一个重要特性是依赖关系。现代浏览器都经过了精心设计，首先请求网页上最重要的元素，以最优的顺序获取资源，由此来优化页面性能。拿到了 HTML 之后，在渲染页面之前，浏览器通常还需要 CSS 和关键 JavaScript 这样的东西。在没有多路复用的时候，在它可以发出对新对象的请求之前，需要等待前一个响应完成。有了 h2，客户端就可以
一次发出所有资源的请求，服务端也可以立即着手处理这些请求。由此带来的问题是，浏览器失去了在 h1 时代默认的资源请求优先级策略。假设服务器同时接收到了 100 个请求，
也没有标识哪个更重要，那么它将几乎同时发送每个资源，次要元素就会影响到关键元素
的传输。 

h2 通过流的依赖关系来解决这个问题。通过 HEADERS 帧和 PRIORITY 帧，客户端可以明确地和服务端沟通它需要什么，以及它需要这些资源的顺序。这是通过声明依赖关系树 和树里的相对权重实现的。

* 依赖关系为客户端提供了一种能力，通过指明某些对象对另一些对象有依赖，告知服务器这些对象应该优先传输。
* 权重让客户端告诉服务器如何确定具有共同依赖关系的对象的优先级。 

我们来看看这个简单的网站

```js
index.html
– header.jpg
– critical.js
– less_critical.js
– style.css
– ad.js
– photo.jpg 
```

在收到主体 HTML 文件之后，客户端会解析它，并生成依赖树，然后给树里的元素分配权重。这时这棵树可能是这样的:


```js
index.html
  – style.css
    – critical.js
      –  less_critical.js (weight 20)
      –  photo.jpg (weight 8)
      –  header.jpg (weight 8)
      –  ad.js (weight 4) 	
```

在这个依赖树里，客户端表明它最需要的是 style.css，其次是 critical.js。没有这两个文件， 它就不能接着渲染页面。等它收到了 critical.js，就可以给出其余对象的相对权重。权重表示服务一个对象时所需要花费的对应“努力”程度。 





## HTTP2.0主要改动

HTTP2.0作为新版协议，改动细节必然很多，不过对应用开发者和服务提供商来说，影响较大的就几点。

#### 多路复用

多路复用允许同时通过单一的 HTTP/2 连接发起多重的 请求/响应 消息，众所周知 ，在 HTTP/1.1 协议中 「浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制。超过限制数目的请求会被阻塞」。

这也是为何一些站点会有多个静态资源 CDN 域名的原因之一，拿 Twitter 为例，[http://twimg.com](http://twimg.com/)，目的就是变相的解决浏览器针对同一域名的请求限制阻塞问题。而 HTTP/2 的多路复用(Multiplexing) 则允许同时通过单一的 HTTP/2 连接发起多重的 请求/响应 消息

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-12.png)

因此 HTTP/2 可以很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息。并行地在同一个 TCP 连接上双向交换消息。

关于多路复用的好处，简单总结就是

* 减少服务端连接压力，减少占用内存，提升连接吞吐量；
* 连接数的减少改善了网络拥塞状况，慢启动时间减少，拥塞和丢包恢复速度更快；
* 避免连接频繁创建和关闭（三次连接、四次挥手）；



#### 首部压缩（Header Compression）

现代网页平均包含 140 个请求，每个 HTTP 请求平均有 460 字节，总数据量达到 63KB。即使在最好的环境下，这也会造成相当长的延时，如果考虑到拥挤的 WiFi 或连接不畅的蜂窝网络，那可是非常痛苦的。这些请求之间通常几乎没有新的或不同的内容，这才是真正的浪费。所以，大家迫切渴望某种类型的压缩。

HTTP/1.1并不支持 HTTP 首部压缩，为此 SPDY 和 HTTP/2 应运而生， SPDY 使用的是通用的 [DEFLATE](https://zh.wikipedia.org/zh-hans/DEFLATE) 算法，而 HTTP/2 则使用了专门为首部压缩而设计的 [HPACK](http://http2.github.io/http2-spec/compression.html) 算法。

![](https://gitee.com/heptaluan/backups/raw/master/cdn/http/11-13.png)

至于为什么不使用 GZIP 压缩

> CRIME 攻击告诉我们，GZIP 也有泄漏加密信息的风
险。CRIME 的原理是这样的，攻击者在请求中添加数据，观察压缩加密后的数据量是否会小于预期。如果变小了，攻击者就知道注入的文本和请求中的其他内容(比如私有的会话 cookie)有重复。在很短的时间内，经过加密
的数据内容就可以全部搞清楚。因此，大家放弃了已有的压缩方案，研发出
HPACK


#### 服务端推送（Server Push）

提升单个对象性能的最佳方式，就是在它被用到之前就放到浏览器的缓存里面。这正是
HTTP/2 的服务端推送的目的。推送使服务器能够主动将对象发给客户端，这可能是因为
它知道客户端不久将用到该对象。 
				
如果服务器决定要推送一个对象(RFC 中称为“推送响应”)，会构造一个 PUSH_PROMISE 帧

🐢PUSH_PROMISE 帧首部中的流 ID 用来响应相关联的请求。推送的响应一定会对应到
客户端已发送的某个请求。如果浏览器请求一个主体 HTML 页面，如果要推送此页面
使用的某个 JavaScript 对象，服务器将使用请求对应的流 ID 构造 PUSH_PROMISE 帧。

						
🦑PUSH_PROMISE 帧的首部块与客户端请求推送对象时发送的首部块是相似的。所以客户端有办法放心检查将要发送的请求。
🐞被发送的对象必须确保是可缓存的。
🐙:method 首部的值必须确保安全。安全的方法就是幂等的那些方法，这是一种不改变任何状态的好办法。例如，GET 请求被认为是幂等的，因为它通常只是获取对象，而POST 请求被认为是非幂等的，因为它可能会改变服务器端的状态。
🐳理想情况下，PUSH_PROMISE 帧应该更早发送，应当早于客户端接收到可能承载着推送对象的 DATA 帧。假设服务器要在发送 PUSH_PROMISE 之前发送完整的 HTML，那客户端可能在接收到 PUSH_PROMISE 之前已经发出了对这个资源的请求。h2 足够健壮，可以优雅地解决这类问题，但还是会有些浪费。
🐡PUSH_PROMISE 帧会指示将要发送的响应所使用的流 ID。

客户端会从 1 开始设置流 ID，之后每新开启一个流，就会增加 2，之后一直
使用奇数。服务器开启在 PUSH_PROMISE 中标明的流时，设置的流 ID 从 2
开始，之后一直使用偶数。这种设计避免了客户端和服务器之间的流 ID 冲
突，也可以轻松地判断哪些对象是由服务端推送的。0 是保留数字，用于连
接级控制消息，不能用于创建新的流。 

如果客户端对 PUSH_PROMISE 的任何元素不满意，就可以按照拒收原因选择重置这个流
(使用 RST_STREAM)，或者发送 PROTOCOL_ERROR(在 GOAWAY 帧中)。常见的情况是缓存中已经有了这个对象。而 PROTOCOL_ERROR 是专门留给 PUSH_PROMISE 涉及的协议层面问题的，比如方法不安全，或者当客户端已经在 SETTINGS 帧中表明自己不接受推送时，仍然进行了推送。值得注意的是，服务器可以在 PUSH_PROMISE 发送后立即启动推送流，因此拒收正在进行的推送可能仍然无法避免推送大量资源。推送正确的资源是不够的，还需要保证只推送正确的资源，这是重要的性能优化手段。 


所以到底如何选择要推送的资源？决策的过程需要考虑到如下方面：

 • 资源已经在浏览器缓存中的概率

• 从客户端看来，这些资源的优先级
• 可用的带宽，以及其他类似的会影响客户端接收推送的资源 

如果用户第一次访问页面时，就能向客户端推送页面渲染所需的关键 CSS 和 JS 资源，那 么服务端推送的真正价值就实现了。不过，这要求服务器端实现足够智能，以避免“推送 承诺”(push promise)与主体 HTML 页面传输竞争带宽。


00000.png


关于 Server Push ，一个常见的问题是,如果客户端早已在缓存中有了一份 copy 怎么办？

因为 Push 本身具有投机性，所以肯定会出现推送过去的东西浏览器不需要的情况。这种情况下，HTTP/2 允许客户端通过 RESET_STREAM 主动取消 Push ，然而这样的话，原本可以用于更好方向的 Push 就白白的浪费掉数据往返的价值。

对此，一个推荐的解决方案是，客户端使用一个简洁的 [Cache Digest](https://tools.ietf.org/html/draft-ietf-httpbis-cache-digest-05) 来告诉服务器，哪些东西已经在缓存，因此服务器也就会知道哪些是客户端所需要的。

因为 Cache Digest 使用的是 [Golumb Compressed Sets](https://www.imperialviolet.org/2011/04/29/filters.html)，浏览器客户端可以通过一个连接发送少于 1K 字节的 Packets 给服务端，通知哪些是已经在缓存中存在的

现在，我们已经解决了 chattiness、额外的 Round Trip 开销、啰嗦首部的数据浪费、inline 以及过去的其他优化行为、最后则是 Push 重复资源带来数据浪费，这和我们理想化的目标越来越近了。

Cache Digests 只是其中一个提案之一， 在 HTTP 社区有着更多其他的解决方案，我们也希望在不久的将来看到他们的身影。






## http/2.0反模式

h1 下的一些性能调优办法在 h2 下会起到反作用


#### 域名拆分

域名拆分是为了利用浏览器对每个域名开启多个连接的能力，以便实现资源的并行下载，
绕过 h1 的串行化下载的限制。对于包含大量小型资源的网站，普遍的做法是拆分域名，
以利用现代浏览器针能对每个域名开启 6 个连接的特性。这样实际上做到了让浏览器并行
发送多个请求，以及充分利用可用带宽的效果。因为 HTTP/2 采取多路复用，所以域名拆
分就不是必要的了，并且反而会让协议力图实现的目标落空。 



#### 资源内联

资源内联包括把 JavaScript、样式，甚至图片插入到 HTML 页面中，目的是省掉加载外部资源所需的新连接以及请求响应的时间。然而，有些 Web 性能的最佳实践不推荐使用内联，因为这样会损失更有价值的特性，比如缓存。如果有同一个页面上的重复访问，缓存通常可以减少请求数(而且能够加速页面渲染)。尽管如此，总体来说，对那些渲染滚动
条以上区域所需的微小资源进行内联处理仍是值得的。

> 事实上有证据表明，在性能较弱的 设备上，缓存对象的好处不够多，把内联资源拆分出来并不划算。使用 h2 时的一般原则是避免内联，但是内联也并不一定毫无价值。 


#### 资源合并

资源合并意味着把几个小文件合并成一个大文件。它与内联很相似，旨在省掉那些加载外部资源的请求响应时间，以及解码 / 执行那些资源所消耗的 CPU 资源。之前针对资源内联
的规则同样适用于资源合并，我们可以使用它来合并非常小的文件(1KB 或更小)，以及
对初始渲染很关键的最简化 JavaScript/CSS 资源。 


#### 禁用cookie的域名
			
通过禁用 cookie 的域名来提供静态资源是一项标准的性能优化最佳实践。尤其是使用 h1
时，你无法压缩首部，而且有些网站使用的 cookie 大小常常超过单个 TCP 数据包的限度。
不过，在 h2 下请求首部使用 HPACK 算法被压缩，会显著减少巨型 cookie(尤其是当它
们在先后请求之间保持不变)的字节数。与此同时，禁用 cookie 的域名需要额外的主机名
称，这意味着将开启更多的连接。

> 如果你正在使用禁用 cookie 的域名，以后有机会你可能得考虑消灭它。如果你确实不需要那些域名，最好删掉它们。省一个字节就是一个字节。 
				
			
#### 生成精灵图
			
目前，生成精灵图仍是一种避免小资源请求过多的技术(你能看到人们乐意做什么来优化
h1)。为了生成精灵图，开发者把较小的图片拼合成较大的图片，然后用 CSS 选择图片中
某个部分展示出来。依据设备及其硬件图形处理能力的不同，精灵图要么非常高效，要么
非常低效。如果用 h2，最佳实践就是避免生成精灵图;主要原因在于，多路复用和首部压缩去掉了大量的请求开销。即便如此，还是有些场景适合使用精灵图。


## 性能优化因人而异 
					
为了最大化 Web 性能，需要在许多变量之间取舍，包括网络条件、设备
处理能力、浏览器能力，还有协议限制。这些组成了我们所说的场景，大多数开发者
的时间远远不够考虑那么多场景。 
					
怎么办?最佳实践的第一原则就是:测试。性能测试与监控是获得最大成果的关键，
HTTP/2 也不例外。观察真实用户数据、详尽分析各种条件、查找问题，然后解决它们。要遵循业界推荐的方式，但也不要陷入过早优化的陷阱。




## 参考

* [HTTP/2](https://hpbn.co/http2/)
* [HTTP/2 for Front-End Developers](https://www.mnot.net/talks/h2fe/)
* [HTTP 2.0 协议详解](https://blog.csdn.net/zqjflash/article/details/50179235)
* [Web 性能权威指南](https://item.jd.com/11444582.html)
* [使用 HTTP/2 提升性能的 7 个建议](https://www.w3ctech.com/topic/1563#tip7sharding)
* [HTTP/2 资料汇总](https://www.zhihu.com/question/34074946)